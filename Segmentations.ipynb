{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f993ca51-41c6-4539-8ada-b0ff29b7f61d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nibabel in /home/jovyan/.local/lib/python3.10/site-packages (5.3.2)\n",
      "Requirement already satisfied: monai in /home/jovyan/.local/lib/python3.10/site-packages (1.4.0)\n",
      "Requirement already satisfied: matplotlib in /home/jovyan/.local/lib/python3.10/site-packages (3.10.1)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.1+cu118)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
      "Requirement already satisfied: importlib-resources>=5.12 in /home/jovyan/.local/lib/python3.10/site-packages (from nibabel) (6.5.2)\n",
      "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.10/dist-packages (from nibabel) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6 in /usr/local/lib/python3.10/dist-packages (from nibabel) (4.12.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/jovyan/.local/lib/python3.10/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/lib/python3/dist-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/lib/python3/dist-packages (from matplotlib) (4.29.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: pillow>=8 in /home/jovyan/.local/lib/python3.10/site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: filelock in /home/jovyan/.local/lib/python3.10/site-packages (from torch) (3.11.0)\n",
      "Requirement already satisfied: sympy in /usr/lib/python3/dist-packages (from torch) (1.9)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.10/dist-packages (from torch) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.10/dist-packages (from torch) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.10/dist-packages (from torch) (11.8.87)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.7.0.84 in /usr/local/lib/python3.10/dist-packages (from torch) (8.7.0.84)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.10/dist-packages (from torch) (11.11.3.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.0.86)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.1.48)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.5.86)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.10/dist-packages (from torch) (11.8.86)\n",
      "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install nibabel monai matplotlib torch numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "578025ec-0fb6-4bbe-b24d-0363e51be5ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n",
      "2025-04-01 09:23:59.809690: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-01 09:23:59.848857: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-01 09:23:59.848880: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-01 09:23:59.848916: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-01 09:23:59.857694: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-01 09:24:00.991648: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved segmentation for patient151_frame01.nii.gz\n",
      "Saved segmentation for patient152_frame01.nii.gz\n",
      "Saved segmentation for patient153_frame01.nii.gz\n",
      "Saved segmentation for patient154_frame01.nii.gz\n",
      "Saved segmentation for patient155_frame01.nii.gz\n",
      "Saved segmentation for patient156_frame01.nii.gz\n",
      "Saved segmentation for patient157_frame01.nii.gz\n",
      "Saved segmentation for patient158_frame01.nii.gz\n",
      "Saved segmentation for patient159_frame01.nii.gz\n",
      "Saved segmentation for patient160_frame01.nii.gz\n",
      "Saved segmentation for patient161_frame01.nii.gz\n",
      "Saved segmentation for patient162_frame01.nii.gz\n",
      "Saved segmentation for patient163_frame01.nii.gz\n",
      "Saved segmentation for patient164_frame01.nii.gz\n",
      "Saved segmentation for patient165_frame01.nii.gz\n",
      "Saved segmentation for patient166_frame01.nii.gz\n",
      "Saved segmentation for patient167_frame01.nii.gz\n",
      "Saved segmentation for patient168_frame01.nii.gz\n",
      "Saved segmentation for patient169_frame01.nii.gz\n",
      "Saved segmentation for patient170_frame01.nii.gz\n",
      "Saved segmentation for patient171_frame01.nii.gz\n",
      "Saved segmentation for patient172_frame01.nii.gz\n",
      "Saved segmentation for patient173_frame01.nii.gz\n",
      "Saved segmentation for patient174_frame01.nii.gz\n",
      "Saved segmentation for patient175_frame01.nii.gz\n"
     ]
    }
   ],
   "source": [
    "# Segmentation on secret test set\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import monai\n",
    "from monai.transforms import Compose, EnsureChannelFirstd, ScaleIntensityd, Resized\n",
    "from monai.networks.nets import UNet\n",
    "from monai.networks.utils import one_hot\n",
    "from torch.utils.data import DataLoader\n",
    "from scipy.ndimage import zoom\n",
    "\n",
    "# Configuration\n",
    "ensemble_model_path = \"ensembleHeartUNet.pt\"\n",
    "save_folder = \"Segmentations_secret\"\n",
    "data_path = r'./Secret3d'\n",
    "\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "def load_nii(img_path):\n",
    "    nimg = nib.load(img_path)\n",
    "    return nimg.get_fdata(), nimg.affine\n",
    "\n",
    "def build_test_dict(data_path):\n",
    "    patient_dirs = [os.path.join(data_path, patient) for patient in os.listdir(data_path) \n",
    "                   if os.path.isdir(os.path.join(data_path, patient))]\n",
    "    \n",
    "    image_paths = []\n",
    "    for patient_folder in patient_dirs:\n",
    "        image_paths.extend(sorted(glob.glob(os.path.join(patient_folder, \"*.nii.gz\"))))\n",
    "    image_paths = sorted(image_paths)\n",
    "\n",
    "    dataset_dicts = []\n",
    "    for img_path in image_paths:\n",
    "        dataset_dicts.append({\"img\": img_path})\n",
    "    return dataset_dicts\n",
    "\n",
    "class LoadHeartData(monai.transforms.Transform):\n",
    "    def __call__(self, sample):\n",
    "        img_vol, img_affine = load_nii(sample['img'])\n",
    "        original_shape = img_vol.shape\n",
    "        images = np.moveaxis(img_vol, -1, 0)\n",
    "        slice_list = []\n",
    "        for i in range(images.shape[0]):\n",
    "            slice_list.append({\n",
    "                'img': images[i].astype(np.float32),\n",
    "                'img_meta_dict': {'affine': img_affine, 'original_shape': original_shape},\n",
    "                'img_path': sample['img']\n",
    "            })\n",
    "        return slice_list\n",
    "\n",
    "def flatten_dataset(dataset_list, transform):\n",
    "    flat_list = []\n",
    "    for data in dataset_list:\n",
    "        flat_list.extend(transform(data))\n",
    "    return flat_list\n",
    "\n",
    "def resize_or_pad_slice(pred_np, original_shape):\n",
    "    pred_shape = pred_np.shape[:2]\n",
    "    if pred_shape != original_shape[:2]:\n",
    "        scaling_factors = (\n",
    "            original_shape[0] / pred_shape[0],\n",
    "            original_shape[1] / pred_shape[1]\n",
    "        )\n",
    "        pred_np_resized = zoom(pred_np, zoom=scaling_factors, order=0)\n",
    "    else:\n",
    "        pred_np_resized = pred_np\n",
    "    return pred_np_resized\n",
    "\n",
    "test_transforms = Compose([\n",
    "    LoadHeartData(),\n",
    "    EnsureChannelFirstd(keys=['img'], channel_dim=\"no_channel\"),\n",
    "    ScaleIntensityd(keys=['img']),\n",
    "    Resized(keys=['img'], spatial_size=(256, 256), mode=['bilinear']),\n",
    "])\n",
    "\n",
    "test_dicts = build_test_dict(data_path)\n",
    "img_path = test_dicts[0]['img']\n",
    "test_flat = flatten_dataset(test_dicts, test_transforms)\n",
    "\n",
    "test_dataset = monai.data.Dataset(data=test_flat)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "class EnsembleModel(torch.nn.Module):\n",
    "    def __init__(self, models):\n",
    "        super(EnsembleModel, self).__init__()\n",
    "        self.models = torch.nn.ModuleList(models)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs_list = [torch.softmax(model(x), dim=1) for model in self.models]\n",
    "        avg_outputs = torch.mean(torch.stack(outputs_list), dim=0)\n",
    "        return avg_outputs\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ensemble_model = torch.load(ensemble_model_path, map_location=device)\n",
    "ensemble_model.to(device)\n",
    "ensemble_model.eval()\n",
    "\n",
    "#%% Inference and Save Segmentations as 3D Volumes\n",
    "with torch.no_grad():\n",
    "    # Dictionary to hold all slices for a scan\n",
    "    scan_segmentations = {}\n",
    "\n",
    "    for idx, sample in enumerate(test_dataset):\n",
    "        img_path = sample[\"img_path\"]\n",
    "        img = sample[\"img\"].to(device).unsqueeze(0)\n",
    "        affine = sample[\"img_meta_dict\"]['affine']\n",
    "        original_shape = sample[\"img_meta_dict\"]['original_shape']\n",
    "\n",
    "        # Run inference with the ensemble model\n",
    "        avg_output = ensemble_model(img)\n",
    "\n",
    "        # Convert outputs to NumPy\n",
    "        avg_output_np = avg_output.squeeze().cpu().numpy()  # Shape: (C, H, W)\n",
    "\n",
    "        # Get the predicted class (argmax of averaged probabilities)\n",
    "        pred = np.argmax(avg_output_np, axis=0).astype(np.uint8)  # Shape: (H, W)\n",
    "\n",
    "        # Resize or pad the slice to match the original shape\n",
    "        pred_resized = resize_or_pad_slice(pred, original_shape)\n",
    "\n",
    "        # Add a singleton dimension for stacking\n",
    "        if len(pred_resized.shape) == 2:\n",
    "            pred_resized = np.expand_dims(pred_resized, axis=-1)\n",
    "\n",
    "        # Extract the base name of the scan from the image path\n",
    "        scan_name = os.path.basename(img_path).split('.')[0]\n",
    "\n",
    "        # Initialize the dictionary for this scan if not already done\n",
    "        if scan_name not in scan_segmentations:\n",
    "            scan_segmentations[scan_name] = {\n",
    "                \"slices\": [],\n",
    "                \"affine\": affine\n",
    "            }\n",
    "\n",
    "        # Append the current slice and uncertainty to the scan's data\n",
    "        scan_segmentations[scan_name][\"slices\"].append(pred_resized)\n",
    "\n",
    "# Save each 3D segmentation\n",
    "for scan_name, scan_data in scan_segmentations.items():\n",
    "    # Retrieve slices, affine, and voxel sizes\n",
    "    segmentation_slices = scan_data[\"slices\"]\n",
    "    affine = scan_data[\"affine\"]\n",
    "\n",
    "    # Stack the segmentation slices to form the 3D volume\n",
    "    segmentation_3d = np.concatenate(segmentation_slices, axis=-1)\n",
    "\n",
    "    # Update the affine matrix with the voxel sizes\n",
    "    new_affine = affine.copy()\n",
    "\n",
    "    # Save the segmentation\n",
    "    output_filename = f\"{scan_name}.nii.gz\"\n",
    "    output_path = os.path.join(save_folder, output_filename)\n",
    "    pred_nii = nib.Nifti1Image(segmentation_3d, new_affine)\n",
    "    nib.save(pred_nii, output_path)\n",
    "\n",
    "    print(f\"Saved segmentation for {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "998d0f04-68ff-4e2e-b331-52090dfb2450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved segmentation for patient101_frame01_gt.nii.gz\n",
      "Saved segmentation for patient101_frame14_gt.nii.gz\n",
      "Saved segmentation for patient102_frame01_gt.nii.gz\n",
      "Saved segmentation for patient102_frame13_gt.nii.gz\n",
      "Saved segmentation for patient103_frame01_gt.nii.gz\n",
      "Saved segmentation for patient103_frame11_gt.nii.gz\n",
      "Saved segmentation for patient104_frame01_gt.nii.gz\n",
      "Saved segmentation for patient104_frame11_gt.nii.gz\n",
      "Saved segmentation for patient105_frame01_gt.nii.gz\n",
      "Saved segmentation for patient105_frame10_gt.nii.gz\n",
      "Saved segmentation for patient106_frame01_gt.nii.gz\n",
      "Saved segmentation for patient106_frame13_gt.nii.gz\n",
      "Saved segmentation for patient107_frame01_gt.nii.gz\n",
      "Saved segmentation for patient107_frame10_gt.nii.gz\n",
      "Saved segmentation for patient108_frame01_gt.nii.gz\n",
      "Saved segmentation for patient108_frame09_gt.nii.gz\n",
      "Saved segmentation for patient109_frame01_gt.nii.gz\n",
      "Saved segmentation for patient109_frame10_gt.nii.gz\n",
      "Saved segmentation for patient110_frame01_gt.nii.gz\n",
      "Saved segmentation for patient110_frame11_gt.nii.gz\n",
      "Saved segmentation for patient111_frame01_gt.nii.gz\n",
      "Saved segmentation for patient111_frame07_gt.nii.gz\n",
      "Saved segmentation for patient112_frame01_gt.nii.gz\n",
      "Saved segmentation for patient112_frame12_gt.nii.gz\n",
      "Saved segmentation for patient113_frame01_gt.nii.gz\n",
      "Saved segmentation for patient113_frame08_gt.nii.gz\n",
      "Saved segmentation for patient114_frame01_gt.nii.gz\n",
      "Saved segmentation for patient114_frame11_gt.nii.gz\n",
      "Saved segmentation for patient115_frame01_gt.nii.gz\n",
      "Saved segmentation for patient115_frame13_gt.nii.gz\n",
      "Saved segmentation for patient116_frame01_gt.nii.gz\n",
      "Saved segmentation for patient116_frame09_gt.nii.gz\n",
      "Saved segmentation for patient117_frame01_gt.nii.gz\n",
      "Saved segmentation for patient117_frame13_gt.nii.gz\n",
      "Saved segmentation for patient118_frame01_gt.nii.gz\n",
      "Saved segmentation for patient118_frame10_gt.nii.gz\n",
      "Saved segmentation for patient119_frame01_gt.nii.gz\n",
      "Saved segmentation for patient119_frame09_gt.nii.gz\n",
      "Saved segmentation for patient120_frame01_gt.nii.gz\n",
      "Saved segmentation for patient120_frame08_gt.nii.gz\n",
      "Saved segmentation for patient121_frame01_gt.nii.gz\n",
      "Saved segmentation for patient121_frame10_gt.nii.gz\n",
      "Saved segmentation for patient122_frame01_gt.nii.gz\n",
      "Saved segmentation for patient122_frame06_gt.nii.gz\n",
      "Saved segmentation for patient123_frame01_gt.nii.gz\n",
      "Saved segmentation for patient123_frame11_gt.nii.gz\n",
      "Saved segmentation for patient124_frame01_gt.nii.gz\n",
      "Saved segmentation for patient124_frame07_gt.nii.gz\n",
      "Saved segmentation for patient125_frame01_gt.nii.gz\n",
      "Saved segmentation for patient125_frame07_gt.nii.gz\n",
      "Saved segmentation for patient126_frame01_gt.nii.gz\n",
      "Saved segmentation for patient126_frame07_gt.nii.gz\n",
      "Saved segmentation for patient127_frame01_gt.nii.gz\n",
      "Saved segmentation for patient127_frame07_gt.nii.gz\n",
      "Saved segmentation for patient128_frame01_gt.nii.gz\n",
      "Saved segmentation for patient128_frame11_gt.nii.gz\n",
      "Saved segmentation for patient129_frame01_gt.nii.gz\n",
      "Saved segmentation for patient129_frame08_gt.nii.gz\n",
      "Saved segmentation for patient130_frame01_gt.nii.gz\n",
      "Saved segmentation for patient130_frame11_gt.nii.gz\n",
      "Saved segmentation for patient131_frame01_gt.nii.gz\n",
      "Saved segmentation for patient131_frame09_gt.nii.gz\n",
      "Saved segmentation for patient132_frame01_gt.nii.gz\n",
      "Saved segmentation for patient132_frame15_gt.nii.gz\n",
      "Saved segmentation for patient133_frame01_gt.nii.gz\n",
      "Saved segmentation for patient133_frame10_gt.nii.gz\n",
      "Saved segmentation for patient134_frame01_gt.nii.gz\n",
      "Saved segmentation for patient134_frame15_gt.nii.gz\n",
      "Saved segmentation for patient135_frame01_gt.nii.gz\n",
      "Saved segmentation for patient135_frame10_gt.nii.gz\n",
      "Saved segmentation for patient136_frame01_gt.nii.gz\n",
      "Saved segmentation for patient136_frame12_gt.nii.gz\n",
      "Saved segmentation for patient137_frame01_gt.nii.gz\n",
      "Saved segmentation for patient137_frame11_gt.nii.gz\n",
      "Saved segmentation for patient138_frame01_gt.nii.gz\n",
      "Saved segmentation for patient138_frame10_gt.nii.gz\n",
      "Saved segmentation for patient139_frame01_gt.nii.gz\n",
      "Saved segmentation for patient139_frame08_gt.nii.gz\n",
      "Saved segmentation for patient140_frame01_gt.nii.gz\n",
      "Saved segmentation for patient140_frame09_gt.nii.gz\n",
      "Saved segmentation for patient141_frame01_gt.nii.gz\n",
      "Saved segmentation for patient141_frame11_gt.nii.gz\n",
      "Saved segmentation for patient142_frame01_gt.nii.gz\n",
      "Saved segmentation for patient142_frame12_gt.nii.gz\n",
      "Saved segmentation for patient143_frame01_gt.nii.gz\n",
      "Saved segmentation for patient143_frame12_gt.nii.gz\n",
      "Saved segmentation for patient144_frame01_gt.nii.gz\n",
      "Saved segmentation for patient144_frame09_gt.nii.gz\n",
      "Saved segmentation for patient145_frame01_gt.nii.gz\n",
      "Saved segmentation for patient145_frame13_gt.nii.gz\n",
      "Saved segmentation for patient146_frame01_gt.nii.gz\n",
      "Saved segmentation for patient146_frame10_gt.nii.gz\n",
      "Saved segmentation for patient147_frame01_gt.nii.gz\n",
      "Saved segmentation for patient147_frame09_gt.nii.gz\n",
      "Saved segmentation for patient148_frame01_gt.nii.gz\n",
      "Saved segmentation for patient148_frame10_gt.nii.gz\n",
      "Saved segmentation for patient149_frame01_gt.nii.gz\n",
      "Saved segmentation for patient149_frame12_gt.nii.gz\n",
      "Saved segmentation for patient150_frame01_gt.nii.gz\n",
      "Saved segmentation for patient150_frame12_gt.nii.gz\n"
     ]
    }
   ],
   "source": [
    "# Segmentation on original ACDC test set\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import monai\n",
    "from monai.transforms import Compose, EnsureChannelFirstd, ScaleIntensityd, Resized, Resize\n",
    "from monai.networks.nets import UNet\n",
    "from monai.networks.utils import one_hot\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "\n",
    "ensemble_model_path = \"ensembleHeartUNet.pt\"\n",
    "save_folder = \"Segmentations_ensemble\"  # Folder to save the segmentations\n",
    "\n",
    "# Create the save folder if it doesn't exist\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "#%% Load NIfTI images\n",
    "def load_nii(img_path):\n",
    "    nimg = nib.load(img_path)\n",
    "    return nimg.get_fdata(), nimg.affine\n",
    "\n",
    "#%% Build a dictionary for the test dataset\n",
    "def build_test_dict(data_path):\n",
    "    image_dir = os.path.join(data_path, \"testing\", \"image\")\n",
    "    mask_dir = os.path.join(data_path, \"testing\", \"segmentation\")\n",
    "\n",
    "    image_paths = sorted(glob.glob(os.path.join(image_dir, \"*.nii.gz\")))\n",
    "    mask_paths = sorted(glob.glob(os.path.join(mask_dir, \"*_gt.nii.gz\")))\n",
    "\n",
    "    mask_dict = {os.path.basename(m).replace(\"_gt\", \"\"): m for m in mask_paths}\n",
    "    dataset_dicts = []\n",
    "    for img_path in image_paths:\n",
    "        filename = os.path.basename(img_path)\n",
    "        mask_path = mask_dict.get(filename, None)\n",
    "        if mask_path and os.path.exists(mask_path):\n",
    "            dataset_dicts.append({\"img\": img_path, \"mask\": mask_path})\n",
    "    return dataset_dicts\n",
    "\n",
    "#%% Custom Transform to Load All Slices of Data\n",
    "class LoadHeartData(monai.transforms.Transform):\n",
    "    def __call__(self, sample):\n",
    "        img_vol, img_affine = load_nii(sample['img'])\n",
    "        mask_vol, _ = load_nii(sample['mask'])\n",
    "        images = np.moveaxis(img_vol, -1, 0)\n",
    "        masks = np.moveaxis(mask_vol, -1, 0)\n",
    "        slice_list = []\n",
    "        for i in range(images.shape[0]):\n",
    "            slice_list.append({\n",
    "                'img': images[i].astype(np.float32),\n",
    "                'mask': masks[i].astype(np.uint8),\n",
    "                'img_meta_dict': {'affine': img_affine},\n",
    "                'mask_meta_dict': {'affine': img_affine},\n",
    "                'img_path': sample['img']  # Store the image path\n",
    "            })\n",
    "        return slice_list\n",
    "\n",
    "#%% Define test transforms\n",
    "test_transforms = Compose([\n",
    "    LoadHeartData(),\n",
    "    EnsureChannelFirstd(keys=['img', 'mask'], channel_dim=\"no_channel\"),\n",
    "    ScaleIntensityd(keys=['img']),\n",
    "    Resized(keys=['img', 'mask'], spatial_size=(256, 256), mode=['bilinear', 'nearest']),\n",
    "])\n",
    "\n",
    "#%% Load and flatten the test dataset\n",
    "test_dicts = build_test_dict(r'./database')\n",
    "def flatten_dataset(dataset_list, transform):\n",
    "    flat_list = []\n",
    "    for data in dataset_list:\n",
    "        flat_list.extend(transform(data))\n",
    "    return flat_list\n",
    "\n",
    "test_flat = flatten_dataset(test_dicts, test_transforms)\n",
    "\n",
    "#%% Create test DataLoader\n",
    "test_dataset = monai.data.Dataset(data=test_flat)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "#%% Define the EnsembleModel globally to avoid pickle issues\n",
    "class EnsembleModel(torch.nn.Module):\n",
    "    def __init__(self, models):\n",
    "        super(EnsembleModel, self).__init__()\n",
    "        self.models = torch.nn.ModuleList(models)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Collect and average the predictions (soft voting)\n",
    "        outputs_list = [torch.softmax(model(x), dim=1) for model in self.models]\n",
    "        avg_outputs = torch.mean(torch.stack(outputs_list), dim=0)\n",
    "        return avg_outputs\n",
    "\n",
    "#%% Load the saved ensemble model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "ensemble_model = torch.load(ensemble_model_path, map_location=device)\n",
    "ensemble_model.to(device)\n",
    "ensemble_model.eval()  # Ensure the model is in evaluation mode\n",
    "\n",
    "#%% Inference and Save Segmentations as 3D Volumes\n",
    "with torch.no_grad():\n",
    "    # Dictionary to hold all slices for a scan\n",
    "    scan_segmentations = {}\n",
    "\n",
    "    for idx, sample in enumerate(test_dataset):\n",
    "        img_path = sample[\"img_path\"]  # Get the image path directly from the sample\n",
    "        img = sample[\"img\"].to(device).unsqueeze(0)  # Add batch dimension\n",
    "        mask = sample[\"mask\"].to(device).unsqueeze(0)  # Keep 1-channel\n",
    "        affine = sample[\"img_meta_dict\"]['affine']  # Retrieve the affine matrix from metadata\n",
    "\n",
    "        # Run inference with the ensemble model\n",
    "        output = ensemble_model(img)\n",
    "        \n",
    "        # Apply softmax\n",
    "        output_prob = torch.softmax(output, dim=1)\n",
    "\n",
    "        # Apply argmax to get the predicted class\n",
    "        pred = torch.argmax(output_prob, dim=1, keepdim=True)\n",
    "\n",
    "        # Convert tensors to numpy for saving\n",
    "        pred_np = pred.squeeze().cpu().numpy()\n",
    "\n",
    "        # Ensure the prediction is of type uint8, which is commonly used for segmentation masks\n",
    "        pred_np = pred_np.astype(np.uint8)\n",
    "\n",
    "        # Check if the prediction is 2D or 3D\n",
    "        if len(pred_np.shape) == 2:\n",
    "            # If it's a 2D array (e.g., for a single slice), add a singleton dimension for the third axis\n",
    "            pred_np = np.expand_dims(pred_np, axis=-1)\n",
    "\n",
    "        # Extract the base name of the scan from the image path (without extension)\n",
    "        scan_name = os.path.basename(img_path).split('.')[0]\n",
    "\n",
    "        # If we haven't processed this scan yet, initialize the list for the 3D segmentation\n",
    "        if scan_name not in scan_segmentations:\n",
    "            scan_segmentations[scan_name] = []\n",
    "\n",
    "        # Append the current slice to the list for this scan\n",
    "        scan_segmentations[scan_name].append(pred_np)\n",
    "\n",
    "    # After processing all slices, save each 3D segmentation for the scans\n",
    "    for scan_name, segmentation_slices in scan_segmentations.items():\n",
    "        # Stack the segmentation slices to form the 3D volume (along the third axis)\n",
    "        segmentation_3d = np.concatenate(segmentation_slices, axis=-1)\n",
    "\n",
    "        # Search for the image path in test_dicts based on the scan_name\n",
    "        img_path = None\n",
    "        for entry in test_dicts:\n",
    "            if os.path.basename(entry['img']).replace(\".nii.gz\", \"\") == scan_name:\n",
    "                img_path = entry['img']\n",
    "                break\n",
    "\n",
    "        if img_path is None:\n",
    "            print(f\"Image path for {scan_name} not found!\")\n",
    "            continue\n",
    "\n",
    "        # Load the original image using the LoadHeartData transform\n",
    "        img_vol, _ = load_nii(img_path)\n",
    "        original_shape = img_vol.shape\n",
    "\n",
    "        # Resize segmentation to match the original 3D image shape\n",
    "        resize_transform = Resize(spatial_size=original_shape, mode='nearest')\n",
    "        resized_segmentation = resize_transform(segmentation_3d[None])[0].astype(np.uint8)\n",
    "\n",
    "        # Create a unique filename for the entire scan\n",
    "        output_filename = f\"{scan_name}_gt.nii.gz\"\n",
    "\n",
    "        output_path = os.path.join(save_folder, output_filename)\n",
    "\n",
    "        # Save the resized 3D segmentation as a NIfTI file\n",
    "        pred_nii = nib.Nifti1Image(resized_segmentation, affine)\n",
    "\n",
    "        # Save the NIfTI file\n",
    "        nib.save(pred_nii, output_path)\n",
    "\n",
    "        print(f\"Saved segmentation for {output_filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
